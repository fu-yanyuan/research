CLIP (Contrastive Language–Image Pre-training) 
---
Paper: [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)  
Blog: [CLIP: Connecting Text and Images](https://openai.com/blog/clip/#rf13)  
Github: [link](https://github.com/openai/CLIP)  
Bilibili: [limu](https://www.bilibili.com/video/BV1SL4y1s7LQ/?spm_id_from=333.788)

It seems that CLIP is good at abstaction, which may contribute to sketchy, such as CLIPasso, CLIPdraw   
[如何评价OpenAI最新的工作CLIP：连接文本和图像，zero shot效果堪比ResNet50？ - hzwer的回答 - 知乎](https://www.zhihu.com/question/438649654/answer/2381466612). 

Also, there are some amazing results that combine CLIP with NeRF. [link](https://www.reddit.com/r/deeplearning/comments/rbbxsv/clip_nerf_explained_zeroshot_textguided_object/)  
[CLIP-NeRF](https://arxiv.org/pdf/2112.05139.pdf).  
[dream deformation](https://arxiv.org/pdf/2112.01455.pdf)    

Anyway, let's have a look at it.  

---------------------

